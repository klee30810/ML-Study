# 1. Introduction

- The field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.
- MNIST dataset : using handcrafted rules or heuristics for distinguishing the digits based on the shapes of the strokes leads to a proliferation of rules and of exceptions to the rules and so on, and invariably gives poor results.
- Far better results can be obtained by adopting a machine learning approach in which a large set of $N$ digits ${x_1,...,x_N}$called a **training set** is used to tune the parameters of an adaptive model.
- The result of running the machine learning algorithm can be expressed as a function $y(x)$ which takes a new digit image $x$ as input and that generates an output vector $y$, encoded in the same way as the target vectors. The precise form of the function $y(x)$ is determined during the **training phase(learning phase)**
- Once the model is trained it can then determine the identity of new digit images, which are said to comprise a **test set**. The ability to categorize correctly new examples that differ from those used for training is known as **generalization.**
- **Supervised Learning** : Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors
    - classification : in which the aim is to assign each input vector to one of a finite number of discrete categories
    - regression : the desired output consists of one or more continuous variables
- **Unsupervised Learning** : the training data consists of a set of input vectors x without any corresponding target values
    - **Clustering** : to discover groups of similar examples within the data
    - **density estimation** : determine the distribution of data within the input space
    - **visualization** : project the data from a high-dimensional space down to two or three dimensions

## 1.1 Example : Polynomial Curve Fitting

---

- Our goal is to exploit this training set in order to make predictions of the value $\hat{t}$ of the target variable for some new value $\hat{x}$ of the input variable. This is intrinsically a difficult problem as we have to generalize from a finite data set.
- Probability theory provides a framework for expressing such uncertainty in a precise and quantitative manner, & decision theory allows us to exploit this probabilistic representation in order to make predictions that are optimal according to appropriate criteria.
- We shall proceed rather informally and consider a simple approach based on curve fitting.

$y(x,w)=w_0+w_1x+w_2x^2+...+w_Mx^M=\displaystyle\sum^M_{j=0}w_jx^j$

→ Functions, such as the polynomial, which are linear in the unknown parameters have important properties and are called **linear models**

- The values of the coefficients will be determined by fitting the polynomial to the training data. Minimizing an **error function** that measures the misfit between the function $y(x,w)$, for any given value of w, and the training set data points.

$E(w)=\frac{1}{2}\displaystyle\sum^N_{n=1}{y(x_m,2)-t_n}^2$

- Because the error function is a quadratic function of the coefficients w, its derivatives with respect to the coefficients will be linear in the elements of $w$, and so the minimization of the error function has a unique solution, denoted by $w$, which can be found in closed form
- There remains the problem of choosing the order $M$ of the polynomial, and as we shall see this will turn out to be an example of an important concept called **model comparison or model selection**

![1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled.png](1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled.png)

- The fitted curve oscillates wildly and gives a very poor representation of the function $\sin(2\pi x)$. This latter behaviour is known as **over-fitting**.
- We see that, as M increases, the magnitude of the coefficients typically gets larger. In particular for the M = 9 polynomial, the coefficients have become finely tuned to the data by developing large positive and negative values so that the corresponding polynomial function matches each of the data points exactly, but between data points (particularly near the ends of the range) the function exhibits the large oscillations observed in Figure 1.4.
- choose the complexity of the model **according to the complexity of the problem being solved.**
- One technique that is often used to control the over-fitting phenomenon in such cases is that of **regularization** which involves adding a penalty term to the error function (1.2) in order to discourage the coefficients from reaching large values.

$(1.2)\quad E(w)=\frac{1}{2}\displaystyle\sum^N_{n=1}{y(x_n,w)-t_n}^2+\frac{\lambda}{2}||w||^2$

where $||w||^2 =w^Tw=w_0^2+...+w^2_M$, and the coefficient $\lambda$ governs the relative
importance of the regularization term compared with the sum-of-squares error term, controlling the effective complexity of the model and the degree of over-fitting.

- **Shrinkage** in the statistics & **weight decay** in neural network
- a separate **validation set**, also called a **hold-out set**, used to optimize the model complexity

## 1.2 Probability Theory

---

- Probability theory provides a consistent framework for the quantification and manipulation of uncertainty and forms one of the central foundations for pattern recognition.
- Simple example

![1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled%201.png](1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled%201.png)

- The probability of selecting the red box is $p(B=r)=4/10$ and the probability of selecting the blue box is $p(B=b)=6/10$.
1. probabilities must lie in the **interval [0, 1]**.
2. If the events are **mutually exclusive** and if they **include all possible outcomes** , then we see that the probabilities for **those events must sum to one**.
- The probability that $X$ will take the value $x_i$ and $Y$ will take the value $y_j$ is written $p(X=x_i, Y=y_j)$ and is called the **joint probability** of $X=x_i \& Y=y_j$

$p(X=x_i,\ Y=y_j)=\frac{n_{ij}}{N}$. *"the probability of X and Y"*

- **marginal probability** : $p(X=x_i)=\displaystyle\sum^L_{j=1}p(X=x_i,Y=y_j)$ → **Sum rule**
- **conditional probability** : $p(Y=y_j|X=x_i)=\frac{n_{ij}}{c_i}$
- **product rule**  *"the probability of Y given X"*

 $p(X=x_i,Y=y_j)=\frac{n_{ij}}{N}=\frac{n_{ij}}{c_i}\cdot\frac{c_i}{N} = p(Y=y_j|X=x_i)p(X=x_i)$

- Bayes' theorem : $p(Y|X)=\frac{p(X|Y)p(Y)}{p(X)}$ ⇒ $p(X)=\sum_Yp(X|Y)p(Y)$
    - prior $P(B)$ : the probability available before we observe the identity of the fruit.
    - posterior $P(B|F)$ : the probability obtained after we have observed F.
- **independent** : the joint distribution of two variables factorizes into the product of the marginals, so that $p(X,Y)=p(X)p(Y)$

### 1.2.1 Probability densities

- **probability density** : the probability of a real-valued variable $x$ falling in the interval $(x,x+\delta x)$ is given by $p(x)\delta x$ for $\delta x \rightarrow 0$, $p(x\in(a,b))=\int^b_ap(x)dx$
    - two must condition $p(x)\geq0,\quad\int^{\infty}_{-\infty}p(x)dx=1$
- **cumulative distribution function** : The probability that x lies in the interval (−∞, z)

$\displaystyle P(z)=\int^z_{-\infty}p(x)dx,\quad p=\int p(x,y)dy, \quad p(x,y)=p(y|x)p(x)$

### 1.2.2 Expectations & Covariances

- **expectation** of $f(x)$ : The average value of some function $f(x)$ under a probability distribution $p(x), \quad E[f]=\sum_x p(x)f(x), \quad\int p(x)f(x)dx,\quad \approx\frac{1}{N}\sum^N_{n=1}f(x_n)$
- **conditional expectatio**n : $E_x[f|y]=\sum_xp(x|y)f(x)$
- **variance** : $var[f]=E[(f(x)-E[f(x)])^2]=E[f(x)^2]-E[f(x)]^2$
- **covariance** : $cov[x,y]=E_{x,y}[(x-E[x])(y-E[y])]=E_{x,y}[xy]-E[x]E[y]$

### 1.2.3 Bayesian probabilities

- **classical or frequentist interpretation of probability :** viewed probabilities in terms of the frequencies of random, repeatable events.
- **bayesian view** : probabilities provide a quantification of uncertainty
- we would like to be able to quantify our expression of uncertainty and make precise revisions of uncertainty in the light of new evidence, as well as subsequently to be able to take optimal actions or decisions as a consequence.
- Bayes’ theorem was used to convert a prior probability into a posterior probability by incorporating the evidence provided by the observed data

$p(w|D)=\frac{p(D|w)p(w)}{p(D)},\ p(D)=\int p(D|w)p(w)dw\quad w:assumption,\ D:observation$

- **likelihood** : how probable the observed data set is for different settings of the parameter vector $w$

$posterior\propto likelihood \times prior$

- In frequentist setting, $w$ is considered to be a fixed parameter, whose value is determined by some form of ‘estimator’, and error bars on this estimate are obtained by considering the distribution of possible data sets D.
- Bayesian setting, there is only a single data set D (namely the one that is actually observed), and the uncertainty in the parameters is expressed through a probability distribution over w.

### 1.2.4 The Gaussian distribution

$N(x|\mu,\sigma^2)=\frac{1}{(2\pi\sigma^2)^{1/2}}exp[-\frac{1}{2\sigma^2}(x-\mu)^2]$

$\mu :mean,\ \sigma^2:variance(std^2),\ \beta=1/\sigma^2:precision$

- properties
    - $N(x|\mu,\sigma^2)>0$
    - $\int^{\infty}_\infty N(x|\mu,\sigma^2)dx=1$
    - $E[x]=\int^{\infty}_\infty N(x|\mu,\sigma^2)xdx=\mu$ : the average value of x under the distribution
    - $E[x^2]=\int^\infty_\infty N(x|\mu,\sigma^2)x^2dx=\mu^2+\sigma^2$
    - $var[x]=E[x^2]-E[x]^2=\sigma^2$
    - The maximum of a distribution is known as its mode. For gaussian, the mode coincides with the mean.
- Gaussian distribution  over a D-dimensional vector $x$ of continuous variables

$N=(x|\mu,\Sigma)=\frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{(1/2)}}exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]$

$D-dimensional\ vector\ \mu:mean,\ D\times D\ matrix\ \Sigma : covariance,\newline |\Sigma|:determinant$

- i.i.d : Independent Identically Distributed ⇒ data points that are drawn independently from the same distribution
- suppose that the observations are drawn independently from a Gaussian distribution whose mean $\mu$ and variance $\sigma^2$ are unknown, and we would like to determine these parameters from the data set.

$(1.53)\ likelihood\ function:\ \displaystyle p(x|\mu,\sigma^2)=\prod^N_{n=1}N(x_n|\mu,\sigma^2)\quad(Gaussian)$

- One common criterion for determining the parameters in a probability distribution using an observed data set is to find the **parameter values that maximize the likelihood function**
- In practice, it is more convenient to maximize the **log of the likelihood function**. Because
the logarithm is a monotonically increasing function of its argument, maximization of the log of a function is equivalent to maximization of the function itself. **Taking the log not only simplifies the subsequent mathematical analysis, but it also helps numerically** because the product of a large number of small probabilities can easily underflow the numerical precision of the computer, and this is resolved by computing instead the sum of the log probabilities.

$(1.54)\ \displaystyle \ln p(x|\mu,\sigma^2)=-\frac{1}{2\sigma^2}\sum^N_{n=1}(x_n-\mu)^2-\frac{N}{2}ln\sigma^2-\frac{N}{2}ln(2\pi)$

- Maximizing (1.54) with respect to $\mu,\ \mu_{ML}=\frac{1}{N}\displaystyle \sum^N_{n=1}x_n\quad (1.55)$
- Maximizing (1.54) with respect to $\sigma,\ \sigma^2_{ML}=\frac{1}{N}\displaystyle \sum^N_{n=1}(x_n-\mu_{ML})^2 \quad (1.56)$
- Note that the bias of the maximum likelihood solution becomes **less significant as the number N of data points increases**, and in the limit $N\rightarrow\infty$ the maximum likelihood solution for the variance equals the true variance of the distribution that generated the data.

### 1.2.5 Curve fitting re-visited

- Assume that, given the value of x, the corresponding value of t has a Gaussian distribution $p(t|x,w,\beta)=N(t|y(x,w),\beta^{-1}   \qquad   (1.60)$

![1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled%202.png](1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled%202.png)

- Use logarithmic likelihood function
- $(1.62)\ \displaystyle \ln p(t|x,w,\beta)=-\frac{\beta}{2}\sum^N_{n=1}[y(x_n,w)-t_n]^2+\frac{N}{2}\ln\beta-\frac{N}{2}\ln(2\pi)$

**Determination of maximum likelihood solution of the polynomial coefficients**

- maximizing (1.62) with respect to $w$ → omit terms that do not depend on $w$
- change into "minimizing negative log likelihood

$(1.63)\qquad\frac{1}{\beta_{ML}}=\displaystyle\sum^N_{n=1}[y(x_n,w_{ML})-t_n]^2$

**Prediction for new values of *x***

- probabilistic model is obtained by substituting the maximum likelihood parameters into (1.60)

$(1.64)\qquad p(t|x,w_{ML},\beta_{ML})=N(t|y(x,w_{ML}),\beta^{-1}_{ML}$

- introduce prior distribution

$(1.65)\qquad p(w|\alpha)=N(w|0,\alpha^{-1}I)=(\frac{\alpha}{2\pi})^{(M+1)/2}exp(-\frac{\alpha}{2}w^Tw)$

$\alpha$: controls the distribution of model parameters ⇒ **hyperparameters**

- Bayesian approach : $p(w|x,t,\alpha,\beta)\propto p(t|x,w,\beta)p(w|\alpha)$
- We can now determine $w$ by finding the most probable value of w given the data, in other words by maximizing the posterior distribution ⇒ **maximum posterior (MAP)**
- Taking the negative logarithm of (1.66) & (1.62), (1.65)...

$(1.67)\qquad\frac{\beta}{2}\displaystyle\sum^N_{n=1}[y(x_n,w)-t_n]^2+\frac{\alpha}{2}w^Tw$

⇒ Maximizing the posterior distribution is equivalent to minimizing
the regularized sum-of-squares error function encountered earlier in the form (1.2), with a regularization parameter given by $\lambda=\alpha/\beta$

### 1.2.6 Bayesian curve fitting

- Bayesian treatment simply corresponds to a **consistent application of the sum
and product rules of probability** (x is new test point, training data x & t )

$(1.68)\qquad p(t|x,x,y)=\int(p(t|x,w)p(w|x,t)dw$

- $p(t|x,w)$ is given by (1.60). $p(w|x,t)$ is the posterior distribution over parameters and can be found by normalizing the right-hand side of (1.66)
- This posterior distribution is a Gaussian and can be evaluated analytically.

$(1.69)\qquad p(t|x,x,t)=N(t|m(x),s^2(x))\newline (1.70)\quad mean\ m(x)=\beta \phi(x)^TS\displaystyle\sum^N_{n=1}\phi(x_n)t_n,\newline(1.71)\quad var\ s^2(x)=\beta^{-1}+\phi(x)^TS\phi(x)\newline(1.72)\quad S^{-1}=\alpha I+\beta\sum^N_{n=1}\phi(x_n)\phi(x)^T$

$\phi(x)$ is defined as a vector with elements $\phi_i(x)=x_i$

- The first term in (1.71) represents the uncertainty in the predicted value of t due to the noise on the target variables
- The Second term arises from the uncertainty in the parameters w and is a consequence
of the Bayesian treatment.

## 1.3 Model Selection

---

- The order of the polynomial controls the number of free parameters in the model and thereby governs the model complexity.
- With regularized least squares, the regularization coefficient λ also controls the effective complexity of the model, whereas for more complex models, such as mixture distributions or neural networks there may be multiple parameters governing complexity.
- the performance on the training set is not a good indicator of predictive performance on unseen data due to the problem of over-fitting. ⇒ **Validation set is needed**
- it may be necessary to keep aside a third **test set** on which the performance of the selected model is finally evaluated.

### Cross validation

![1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled%203.png](1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled%203.png)

- allows (S-1)/s of available data do be used for training while making use of all the data to assess performance.
- When data is particularly scarce, it may be appropriate to consider the case S = N, where N is the total number of data points, which gives the **leave-one-out technique**
- Drawbacks
1. the number of training runs that must be performed is increased by a factor of S, and this can be problematic for models in which the training is itself computationally expensive.
2. Exploring combinations of settings for such parameters could, in the worst case, require a number of training runs that is exponential in the number of parameters

⇒ We therefore need to find a measure of performance which **depends only on the training data** and which does not suffer from bias **due to over-fitting.**

- Information criteria are used to correct for the bias of maximum likelihood by the addition of a penalty term to compensate for the over-fitting of more complex models.

ex) AIC, BIC

## 1.4 The Curse of Dimensionality

---

### Problem of high-dimensional spaces

- It can be dealt properly...
1. real data will often **be confined** to a region of the space having lower effective dimensionality
2. real data will typically exhibit some smoothness properties (at least locally) so that for the most part small changes in the input variables will produce small changes in the target variables, and so we **can exploit local interpolation-like techniques**

## 1.5 Decision Theory

---

- Decision theory allows us to make optimal decisions in situations involving uncertainty
- **inference** : determination of $p(x,t)$ from a set of training data
- specific prediction for the value of t, or more generally take a specific action **based on our understanding of the values $t$** is likely to take

$(1.77)\quad p(C_k|x)=\frac{p(x|C_k)p(C_k)}{p(x)}$

- Any of the quantities appearing in Bayes’ theorem can be **obtained from the joint distribution $p(x,C_k)$** by either marginalizing or conditioning with respect to the appropriate variables.

### 1.5.1 Minimizing the misclassification rate

- Assume rule to assign classes : divide the input space into regions $R_k$ called **decision regions**, one for each class, such that all points in $R_k$ are assigned to class $C_k$. The boundaries between decision regions are called **decision boundaries or decision surfaces**
- When there exist only 2 cases...

$(1.78)\quad p(mistake)=p(x\in R_1,C_2)+p(x\in R_2,C_1)\newline=\int_{R_1}p(x,C_2)dx+\int_{R_2}p(x,C_1)dx$

![1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled%204.png](1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled%204.png)

$(1.79)\quad p(correct)=\displaystyle\sum^K_{k=1}p(x\in R_k, C_k)=\sum^K_{k=1}\int_{R_k}p(x,C_k)dx$

- Using the product rule $p(x,C_k)=p(C_k|x)p(x)$, and $p(x)$ is common to all terms,

each x should be assigned to the class having the largest posterior probability $p(C_k|x)$.

### 1.5.2 Minimizing the expected loss

- We can formalize such issues through the introduction of a **loss function**, also called a **cost function**, which is a single, overall measure of loss incurred in taking any of the available decisions or actions. Our goal is then to minimize the total loss incurred.
- The optimal solution is the one which minimizes the loss function.

$(1.80) \quad E[L]=\displaystyle\sum_k\sum_j\int_{R_j}L_{kj}p(x,C_k)dx$

- use the product rule $p(x,C_k)=p(C_k|x)p(x)$ to eliminate the common factor of p(x).

⇒ minimizes $\displaystyle\sum_kL_{kj}p(C_k|x)\qquad(1.81)$

### 1.5.3 The reject option

- **Reject option** : to avoid making decisions on the difficult cases in anticipation of a lower error rate on those examples for which a classification decision is made.

### 1.5.4 Inference & Decision

- **inference stage** : in which we use training data to learn a model for $p(C_k|x)$
- **decision stage** : in which we use these posterior probabilities to make optimal class assignments
- **discriminant function** : to solve both problems together and simply learn a function that maps inputs x directly into decisions

**Solving with three approaches**

1. Solve the inference problem of determining the class-conditional densities $p(x|C_k)$ for each class $C_k$ individually. Also separately infer the prior class probabilities $p(C_k)$. Then use Bayes' theorem $p(C_k|x)=\frac{p(x|C_k)p(C_k)}{p(x)} \quad(1.82)$ to find posterior. Denminator can be found in terms of numerator $p(x)=\sum_kp(x|C_k)p(C_k)\quad(1.83)$
2. Solve the inference problem of determining the posterior class probabilities $p(C_k|x)$, and then subsequently use decision theory to assign each new x to one of the classes**(discriminative models**: model posterior directly)
3. Find a function $f(x)$, called a **discriminant function**, which maps each input x directly onto a class label
- generative models : Approaches that explicitly or implicitly model the distribution of inputs as well as outputs because by sampling from them it is possible to generate synthetic data points in the input space.

→ 1. is the most demanding because it involves finding the joint distribution over both $x \& C_k$, but it allows the marginal density of data $p(x)$ to be determined from (1.83)

→ 2. $p(C_k|x)$ can be obtained directly.

→ 3. combining the inference and decision stages into a single learning problem, but 

we no longer have access to the posterior probabilities $p(C_k|x)$.

### 1.5.5 Loss functions for regression

- The decision stage consists of choosing a specific estimate $y(x)$ of the value of $t$ for each input $x$

$(1.86)\quad E[L]=\int\int L(t,y(x))p(x,t)dxdt=\int\int [y(x)-t]^2p(x,t)dxdt \newline \frac{\delta E[L]}{\delta y(x)}=2\int[y(x)-t]p(x,t)dt=0 \rightarrow y(x)=\frac{\int tp(x,t)dt}{p(x)}=\int tp(t|x)dt=E_t[t|x]$

→ the conditional average of t conditioned on x : **regression function**

⇒ the optimal solution is the conditional average $y(x)=E_t[t|x]$

![1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled%205.png](1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled%205.png)

$[y(x)-t]^2=[y(x)-E[t|x]+E[t|x]-t]^2\newline=[y(x)-E[t|x]]^2+2[y(x)-E[t|x]][E[t|x]-t]+[E[t|x]-t]^2$

($E[t|x]=E_t[t|x]$)

→ $(1.90)\quad E[L]=\displaystyle \int[y(x)-E[t|x]]^2p(x)dx+\int[E[t|x]-t]^2p(x)dx$

- The function $y(x)$ we seek to determine enters **only in the first term**, which will be
minimized when $y(x)$ is equal to $E[t|x]$, in which case this term will vanish.
- The second term is the variance of the distribution of t, averaged over x. It represents the **intrinsic variability** of the target data and can be regarded as **noise**.

## 1.6 Information Theory

---

- The amount of information can be viewed as the **‘degree of surprise’** on learning the
value of $x$.

$(1.92)\quad h(x)=-\log_2p(x)$

- Negative sign ensures that information is positive or zero. **Note that low probability events $x$ correspond to high information content.**

$(1.93)\quad H[x]=-\displaystyle\sum_x p(x)\log_2p(x)$

- The average amount of information that they transmit in the process : **Entropy**
- From example, the nonuniform distribution has a smaller entropy than the uniform one.
- Entropy is addressed the average amount of information needed **to specify the state of a random variable**
- Distributions $p(x_i)$that are sharply peaked around a few values will have a relatively low entropy, whereas those that are spread more evenly across many values will have higher entropy.

![1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled%206.png](1%20Introduction%20dfb58cd8b1864be485fc05e330f26aa6/Untitled%206.png)

**Entropy When $p(x)$ is continuous**

$(1.101)\quad\displaystyle\int^{(i+1)\triangle}_{i\triangle}p(x)dx=p(x_i)\triangle$ : mean value theorem

$(1.102)\quad H_{\triangle}=\displaystyle-\sum_ip(x_i)\triangle\ln(p(x_i)\triangle)=-\sum_ip(x_i)\triangle\ln p(x_i)-\ln\triangle$

$(1.103)\quad\displaystyle\lim_{\triangle\rightarrow 0}\{\sum_ip(x_i)\triangle\ln p(x_i)\}=\int p(x)\ln p(x)dx$ : differential entropy

- We see that the discrete and continuous forms of the entropy differ by a quantity $\ln\triangle$, which
diverges in the limit Δ → 0. This reflects the fact that **to specify a continuous variable very precisely requires a large number of bits.**

**Maximum entropy configuration for a continuous variable**

- maximize the differential entropy with three constrains...

$(1.105)\quad\displaystyle\int^\infty_{-\infty}p(x)dx=1$

$(1.106)\quad\displaystyle\int^\infty_{-\infty}xp(x)dx=\mu$

$(1.107)\quad\displaystyle\int^\infty_{-\infty}(x-\mu)^2p(x)dx=\sigma^2$

- perform Lagrange Multiplier so that we maximize the following functional with respect to $(x)$

$-\displaystyle\int^\infty_{-\infty}p(x)\ln p(x)dx+\lambda_1(\int^\infty_{-\infty}p(x)dx-1)+\lambda_2(\int^\infty_{-\infty}xp(x)dx-\mu) + \lambda_3(\int^\infty_{-\infty}(x-\mu)^2p(x)dx-\sigma^2)$

- set the derivative of this functional to zero

$(1.108)\quad p(x)=exp\{-1+\lambda_1+\lambda_2x+\lambda_3(x-\mu)^2\}$

- back substitution into three constraint equations

$(1.109)\quad p(x)=\frac{1}{(2\pi\sigma^2)^{1/2}}exp\{-\frac{(x-\mu)^@}{2\sigma^2}\}$ 

⇒ the distribution that maximizes the differential entropy is the **Gaussian**

$(1.110)\quad H[x]\frac{1}{2}\{1+\ln(2\pi\sigma^2)\}$ : entropy increases as the distribution becomes broader ($\sigma^2$↑)

$(1.111)\quad H[y|x]=-\int\int p(y,x)\ln p(y|x)dydx$

$(1.112)\quad H[x,y]=H[y|x]+H[x]$

⇒ Thus the information needed to describe x and y is given by the sum of the information needed to describe x alone plus the additional information required to specify y given x.

### 1.6.1 Relative entropy & mutual information

- unknown distribution $p(x)$, and we have modeled this using an approximating distribution $q(x)$. Use $q(x)$ to transmit values of x to a receiver, then then the average additional amount of information (in nats) required to specify the value of x (assuming we choose an efficient coding scheme) as a result of using q(x) instead of the true distribution p(x) is given by

$(1.113)\quad KL(p||q)=-\displaystyle\int p(x)\ln q(x)dx-(-\int p(x)\ln p(x)dx)\newline\qquad=-\int p(x)\ln\{\frac{q(x)}{p(x)}\}dx$

⇒ **Kullback-Leibler divergence (KL divergence)**

- not a symmetrical quantity
- $KL(p||q)\geq0$ if $p(x)=q(x)$
    - $f(x)$ is convex if it has the property that every chord lies on or above the function.
    - Any value of x in the interval from $x=a$ to .$x=b$ can be written in the form where $0\leq\lambda\leq1$. Convexity then implies $f(\lambda a+(1-\lambda)b)\leq\lambda f(a)+(1-\lambda)f(b)$

    → equivalent to the requirement that the second derivative of the function be everywhere positive.

    - from (1.114) $f(\displaystyle\sum^M_{i=1}\lambda_ix_i)\leq\int f(x)p(x)dx)\quad (1.115)$ : **Jensen's Inequality**
    - interpret $\lambda_i$ as the probability distribution over discrete $x$ taking ${x_i}$

    $(1.116)\quad f(E[])\leq E[f(x)]$)

    $(1.117)\quad f(\int xp(x)dx)\leq\int f(x)p(x)dx$

    - apply (1.117) to KL divergence (1.113)

    $(1.118)\quad KL(p||q)=0\int p(x)\ln\{\frac{q(x)}{p(x)}\}dx\geq-\ln\int q(x)dx=0$

    - In fact, $-\ln x$ is a strictly convex function, so the equality will hold if, and only if, $q(x)=p(x)$ for all x. Thus we can interpret the Kullback-Leibler divergence as a **measure of the dissimilarity of the two distributions $p(x)$ and $q(x)$**.
- If we use a distribution that is different from the true one, then we must necessarily have a less efficient coding, and on average the additional information that must be transmitted is (at least) equal to the Kullback-Leibler divergence between the two distributions.

- Try to approximate unknown distribution $p(x)$ using some parametric distribution $q(x|\theta)$
    - One way to determine $\theta$ is to minimize the Kullback-Leibler divergence between $p(x)$and $q(x|\theta)$with respect to $\theta$. The expectation with respect to $p(x)$ can be approximated by a finite sum over these points, using (1.35), so that

    $(1.119)\quad KL(p||q)\approx\displaystyle\sum^N_{n=1}\{-\ln q(x_n|\theta)+\ln p(x_n)\}$

    - The second term on the right-hand side of (1.119) is independent of $\theta$, and the first
    term is the negative log likelihood function for $\theta$ under the distribution $q(x|\theta)$ evaluated using the training set.

    **⇒ minimizing KL divergence = maximizing likelihood**

- KL divergence between the joint distribution & the product of the marginals...

$(1.120)\quad I[x,y]=KL(p(x,y)||p(x)p(y))=-\int\int p(x,y)\ln(\frac{p(x)p(y)}{p(x,y)})dxdy$

⇒ **mutual information**

- $I(x,y)\geq0$ if x & y are independent

$I[x,y]=H[x]-H[x|y]=H[y]-H[y|x]$

⇒ Thus we can view the mutual information as **the reduction in the uncertainty about x
by virtue of being told the value of y (or vice versa).**

- From a Bayesian perspective, we can view $p(x)$ as the **prior** distribution for x and $p(x|y)$ as the **posterior** distribution after we have observed new data y. The mutual information **therefore represents the reduction in uncertainty about $x$ as a consequence of the new observation $y$**